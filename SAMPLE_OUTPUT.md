---
AIGC:
    ContentProducer: Minimax Agent AI
    ContentPropagator: Minimax Agent AI
    Label: AIGC
    ProduceID: "00000000000000000000000000000000"
    PropagateID: "00000000000000000000000000000000"
    ReservedCode1: 3046022100c1fb96c9a8e4d7a4ff5522afd870698e3b56c69819e292647a9f871bc64e9fc1022100fbd0450248a7cc5a176e7c49a96426b508db0d0b62e444278ec5925bc6f0d6d3
    ReservedCode2: 30450221008fa477cb541d124d51882d379610ac669b19144aebac8ccb1eb583c62effeb1e02206f1b375233fd09da7d3ff9a89bce65954d946c2e86750adf76c6c811af7850ab
---

# ç¤ºä¾‹è¾“å‡ºæ–‡ä»¶

## ç¤ºä¾‹: summary.md

```markdown
# GitHub Issues è¸©å‘æŠ¥å‘Šæ‘˜è¦

## ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ

- **æŠ“å–æ—¶é—´**: 2025-12-11 23:51:39
- **æ€»è®¡é—®é¢˜æ•°**: 156
- **æ¶‰åŠä»“åº“æ•°**: 7

## ğŸ¢ ä»“åº“ç»Ÿè®¡

- **vllm-project/vllm**: 32 ä¸ªé«˜ä»·å€¼é—®é¢˜
- **sgl-project/sglang**: 28 ä¸ªé«˜ä»·å€¼é—®é¢˜
- **NVIDIA/TensorRT-LLM**: 25 ä¸ªé«˜ä»·å€¼é—®é¢˜
- **microsoft/DeepSpeed**: 22 ä¸ªé«˜ä»·å€¼é—®é¢˜
- **pytorch/pytorch**: 19 ä¸ªé«˜ä»·å€¼é—®é¢˜
- **huggingface/transformers**: 17 ä¸ªé«˜ä»·å€¼é—®é¢˜
- **ray-project/ray**: 13 ä¸ªé«˜ä»·å€¼é—®é¢˜

## ğŸ¯ é«˜ä»·å€¼é—®é¢˜ç±»åˆ«åˆ†å¸ƒ

- **æ€§èƒ½é—®é¢˜**: 45 ä¸ªé—®é¢˜
- **GPUå†…å­˜é—®é¢˜**: 38 ä¸ªé—®é¢˜
- **åˆ†å¸ƒå¼è®­ç»ƒ**: 31 ä¸ªé—®é¢˜
- **æ¨¡å‹æ¨ç†**: 24 ä¸ªé—®é¢˜
- **å´©æºƒé”™è¯¯**: 12 ä¸ªé—®é¢˜
- **å†…å­˜æ³„æ¼**: 6 ä¸ªé—®é¢˜

## ğŸ“‹ è¯¦ç»†æŠ¥å‘Š

- [vllm-project_vllm](./vllm-project_vllm.md)
- [sgl-project_sglang](./sgl-project_sglang.md)
- [NVIDIA_TensorRT-LLM](./NVIDIA_TensorRT-LLM.md)
- [microsoft_DeepSpeed](./microsoft_DeepSpeed.md)
- [pytorch_pytorch](./pytorch_pytorch.md)
- [huggingface_transformers](./huggingface_transformers.md)
- [ray-project_ray](./ray-project_ray.md)

---

*æŠ¥å‘Šç”± gh-pitfall-scraper è‡ªåŠ¨ç”Ÿæˆ*
```

## ç¤ºä¾‹: vllm-project_vllm.md

```markdown
# vllm-project/vllm - é«˜ä»·å€¼å·¥ç¨‹é—®é¢˜æŠ¥å‘Š

## ğŸ“ˆ é—®é¢˜æ¦‚è§ˆ

- **ç”Ÿæˆæ—¶é—´**: 2025-12-11 23:51:39
- **é—®é¢˜æ€»æ•°**: 32
- **å¹³å‡è¯„åˆ†**: 67.5

---

## 1. Performance regression in GPU memory usage after v0.4.0

**é“¾æ¥**: [https://github.com/vllm-project/vllm/issues/2847](https://github.com/vllm-project/vllm/issues/2847)  
**è¯„åˆ†**: 85.0/100  
**çŠ¶æ€**: open  
**åˆ›å»ºæ—¶é—´**: 2025-12-01  
**æ›´æ–°æ—¶é—´**: 2025-12-09  

**æ ‡ç­¾**: [bug](https://github.com/vllm-project/vllm/labels/bug) [performance](https://github.com/vllm-project/vllm/labels/performance)

**è¯„åˆ†ç†ç”±**:
- å…³é”®è¯åŒ¹é…: 25.0åˆ†
- æ¨¡å¼åŒ¹é…: 20.0åˆ†
- æ ‡ç­¾åŒ¹é…: 15.0åˆ†
- çŠ¶æ€è¯„åˆ†: 10.0åˆ†
- æ´»è·ƒåº¦è¯„åˆ†: 15.0åˆ†

**é—®é¢˜æè¿°**:
```
After upgrading to v0.4.0, we're seeing significant memory usage increase during inference. The GPU memory consumption has increased by approximately 30% compared to v0.3.0 when processing the same batch size and model configuration.

Environment:
- CUDA 12.1
- A100 80GB
- Model: llama-2-70b
- Batch size: 16
- Sequence length: 2048

This is blocking our production deployment...
```

---

## 2. CUDA kernel crash when using flash attention with large batch sizes

**é“¾æ¥**: [https://github.com/vllm-project/vllm/issues/2756](https://github.com/vllm-project/vllm/issues/2756)  
**è¯„åˆ†**: 82.0/100  
**çŠ¶æ€**: open  
**åˆ›å»ºæ—¶é—´**: 2025-11-28  
**æ›´æ–°æ—¶é—´**: 2025-12-10  

**æ ‡ç­¾**: [critical](https://github.com/vllm-project/vllm/labels/critical) [cuda](https://github.com/vllm-project/vllm/labels/cuda)

**è¯„åˆ†ç†ç”±**:
- å…³é”®è¯åŒ¹é…: 30.0åˆ†
- æ¨¡å¼åŒ¹é…: 25.0åˆ†
- æ ‡ç­¾åŒ¹é…: 12.0åˆ†
- çŠ¶æ€è¯„åˆ†: 10.0åˆ†
- æ´»è·ƒåº¦è¯„åˆ†: 5.0åˆ†

**é—®é¢˜æè¿°**:
```
The application crashes with CUDA error when batch size exceeds 32 when using Flash Attention. Error message:

CUDA kernel launch failed: misaligned address
CUDA error: misaligned address

This happens consistently with:
- Model: mixtral-8x7b-instruct
- Flash attention enabled
- Batch size > 32
- Sequence length > 1024
```

---

## 3. Memory leak in distributed training mode

**é“¾æ¥**: [https://github.com/vllm-project/vllm/issues/2691](https://github.com/vllm-project/vllm/issues/2691)  
**è¯„åˆ†**: 78.0/100  
**çŠ¶æ€**: open  
**åˆ›å»ºæ—¶é—´**: 2025-11-25  
**æ›´æ–°æ—¶é—´**: 2025-12-08  

**æ ‡ç­¾**: [bug](https://github.com/vllm-project/vllm/labels/bug) [distributed](https://github.com/vllm-project/vllm/labels/distributed)

**è¯„åˆ†ç†ç”±**:
- å…³é”®è¯åŒ¹é…: 28.0åˆ†
- æ¨¡å¼åŒ¹é…: 20.0åˆ†
- æ ‡ç­¾åŒ¹é…: 10.0åˆ†
- çŠ¶æ€è¯„åˆ†: 10.0åˆ†
- æ´»è·ƒåº¦è¯„åˆ†: 10.0åˆ†

**é—®é¢˜æè¿°**:
```
Memory usage keeps increasing during multi-node training. The memory leak is more severe when using tensor parallelism across multiple GPUs.

Environment:
- 4x A100 80GB
- NCCL backend
- Ray cluster
- Model: llama-70b

After 2-3 hours of training, we see consistent 2-3GB memory increase per GPU...
```

---

*æŠ¥å‘Šç”± gh-pitfall-scraper ç”Ÿæˆäº 2025-12-11 23:51:39*
```

## ç¤ºä¾‹: summary.json

```json
{
  "generated_at": "2025-12-11T23:51:39Z",
  "total_repos": 7,
  "total_issues": 156,
  "repository_stats": {
    "vllm-project/vllm": {
      "issue_count": 32,
      "avg_score": 67.5
    },
    "sgl-project/sglang": {
      "issue_count": 28,
      "avg_score": 64.2
    },
    "NVIDIA/TensorRT-LLM": {
      "issue_count": 25,
      "avg_score": 71.8
    },
    "microsoft/DeepSpeed": {
      "issue_count": 22,
      "avg_score": 69.3
    },
    "pytorch/pytorch": {
      "issue_count": 19,
      "avg_score": 58.7
    },
    "huggingface/transformers": {
      "issue_count": 17,
      "avg_score": 55.1
    },
    "ray-project/ray": {
      "issue_count": 13,
      "avg_score": 62.4
    }
  }
}
```

## ç¤ºä¾‹: scraping_summary.txt

```
GitHub Issues è¸©å‘å†…å®¹æŠ“å–æŠ¥å‘Š
=====================================

æŠ“å–æ—¶é—´: 2025-12-11 23:51:39
é…ç½®æ–‡ä»¶: config.yaml

ä»“åº“ç»Ÿè®¡:
- vllm-project/vllm: 32/89 é—®é¢˜ (è¿‡æ»¤ç‡: 36.0%)
- sgl-project/sglang: 28/76 é—®é¢˜ (è¿‡æ»¤ç‡: 36.8%)
- NVIDIA/TensorRT-LLM: 25/67 é—®é¢˜ (è¿‡æ»¤ç‡: 37.3%)
- microsoft/DeepSpeed: 22/58 é—®é¢˜ (è¿‡æ»¤ç‡: 37.9%)
- pytorch/pytorch: 19/45 é—®é¢˜ (è¿‡æ»¤ç‡: 42.2%)
- huggingface/transformers: 17/39 é—®é¢˜ (è¿‡æ»¤ç‡: 43.6%)
- ray-project/ray: 13/31 é—®é¢˜ (è¿‡æ»¤ç‡: 41.9%)

æ€»è®¡: 7 ä¸ªä»“åº“, 405 ä¸ªé—®é¢˜, 156 ä¸ªé«˜ä»·å€¼é—®é¢˜
è¿‡æ»¤ç‡: 38.5%

å·¥å…·: gh-pitfall-scraper
```